{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import typing\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "\n",
    "def prepare_dataloader(\n",
    "        dataframe: pd.DataFrame,\n",
    "        target_datetimes: typing.List[datetime.datetime],\n",
    "        stations: typing.Dict[typing.AnyStr, typing.Tuple[float, float, float]],\n",
    "        target_time_offsets: typing.List[datetime.timedelta],\n",
    "        config: typing.Dict[typing.AnyStr, typing.Any],\n",
    ") -> tf.data.Dataset:\n",
    "    \"\"\"This function should be modified in order to prepare & return your own data loader.\n",
    "    Note that you can use either the netCDF or HDF5 data. Each iteration over your data loader should return a\n",
    "    2-element tuple containing the tensor that should be provided to the model as input, and the target values. In\n",
    "    this specific case, you will not be able to provide the latter since the dataframe contains no GHI, and we are\n",
    "    only interested in predictions, not training. Therefore, you must return a placeholder (or ``None``) as the second\n",
    "    tuple element.\n",
    "    Reminder: the dataframe contains imagery paths for every possible timestamp requested in ``target_datetimes``.\n",
    "    However, we expect that you will use some of the \"past\" imagery (i.e. imagery at T<=0) for any T in\n",
    "    ``target_datetimes``, but you should NEVER rely on \"future\" imagery to generate predictions (for T>0). We\n",
    "    will be inspecting data loader implementations to ensure this is the case, and those who \"cheat\" will be\n",
    "    dramatically penalized.\n",
    "    See https://github.com/mila-iqia/ift6759/tree/master/projects/project1/evaluation.md for more information.\n",
    "    Args:\n",
    "        dataframe: a pandas dataframe that provides the netCDF file path (or HDF5 file path and offset) for all\n",
    "            relevant timestamp values over the test period.\n",
    "        target_datetimes: a list of timestamps that your data loader should use to provide imagery for your model.\n",
    "            The ordering of this list is important, as each element corresponds to a sequence of GHI values\n",
    "            to predict. By definition, the GHI values must be provided for the offsets given by ``target_time_offsets``\n",
    "            which are added to each timestamp (T=0) in this datetimes list.\n",
    "        stations: a map of station names of interest paired with their coordinates (latitude, longitude, elevation).\n",
    "        target_time_offsets: the list of timedeltas to predict GHIs for (by definition: [T=0, T+1h, T+3h, T+6h]).\n",
    "        config: configuration dictionary holding any extra parameters that might be required by the user. These\n",
    "            parameters are loaded automatically if the user provided a JSON file in their submission. Submitting\n",
    "            such a JSON file is completely optional, and this argument can be ignored if not needed.\n",
    "    Returns:\n",
    "        A ``tf.data.Dataset`` object that can be used to produce input tensors for your model. One tensor\n",
    "        must correspond to one sequence of past imagery data. The tensors must be generated in the order given\n",
    "        by ``target_sequences``.\n",
    "    \"\"\"\n",
    "    ################################## MODIFY BELOW ##################################\n",
    "    # WE ARE PROVIDING YOU WITH A DUMMY DATA GENERATOR FOR DEMONSTRATION PURPOSES.\n",
    "    # MODIFY EVERYTHINGIN IN THIS BLOCK AS YOU SEE FIT\n",
    "\n",
    "    def dummy_data_generator():\n",
    "        \"\"\"\n",
    "        Generate dummy data for the model, only for example purposes.\n",
    "        \"\"\"\n",
    "        batch_size = 32\n",
    "        image_dim = (64, 64)\n",
    "        n_channels = 5\n",
    "        output_seq_len = 4\n",
    "\n",
    "        for i in range(0, len(target_datetimes), batch_size):\n",
    "            batch_of_datetimes = target_datetimes[i:i+batch_size]\n",
    "            samples = tf.random.uniform(shape=(\n",
    "                len(batch_of_datetimes), image_dim[0], image_dim[1], n_channels\n",
    "            ))\n",
    "            targets = tf.zeros(shape=(\n",
    "                len(batch_of_datetimes), output_seq_len\n",
    "            ))\n",
    "            # Remember that you do not have access to the targets.\n",
    "            # Your dataloader should handle this accordingly.\n",
    "            yield samples, targets\n",
    "\n",
    "    data_loader = tf.data.Dataset.from_generator(\n",
    "        dummy_data_generator, (tf.float32, tf.float32)\n",
    "    )\n",
    "\n",
    "    ################################### MODIFY ABOVE ##################################\n",
    "\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "prepare_dataloader() missing 3 required positional arguments: 'stations', 'target_time_offsets', and 'config'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-e2234eb12954>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcatalog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tests/data/catalog-test.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcatalog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatalog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: prepare_dataloader() missing 3 required positional arguments: 'stations', 'target_time_offsets', and 'config'"
     ]
    }
   ],
   "source": [
    "\n",
    "import pickle\n",
    "catalog = pickle.load(open(\"tests/data/catalog-test.pkl\",\"rb\"))\n",
    "g = prepare_dataloader(catalog, catalog.index.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
